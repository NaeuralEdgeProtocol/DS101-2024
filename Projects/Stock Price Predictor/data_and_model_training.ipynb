{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import requests \n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import traceback\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer  # News sentiment analysis\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, LSTM, Conv1D, MaxPooling1D, concatenate, \n",
    "    Flatten, Dropout, Bidirectional, Activation, RepeatVector,\n",
    "    Permute, TimeDistributed, multiply\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber as huber_loss\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Stock symbols to process\n",
    "STOCK_SYMBOLS = [\"AAPL\", \"TSLA\", \"UNP\", \"META\", \"MSFT\", \"NVDA\", \"XOM\", \"INTC\", \"AVGO\", \"PLTR\", \"NXPI\"]\n",
    "NEWS_API_KEY = \"2025548ba34a4294a0f3c18c36311f39\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch news data\n",
    "def fetch_news(api_key, query):\n",
    "    url = f\"https://newsapi.org/v2/everything?q={query}&apiKey={api_key}\"\n",
    "    print(f\"Fetching news from URL: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            news_data = response.json()\n",
    "            if news_data.get('articles'):\n",
    "                return news_data\n",
    "            \n",
    "        else:\n",
    "            print(f\"Failed to fetch news for {query}: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news: {str(e)}\")\n",
    "    return {\"status\": \"error\", \"totalResults\": 0, \"articles\": []}\n",
    "\n",
    "def preprocess_news(articles):\n",
    "    if not articles:\n",
    "        return pd.DataFrame(columns=['date', 'sentiment'])\n",
    "        \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    processed_data = []\n",
    "    \n",
    "    for article in articles:\n",
    "        try:\n",
    "            if not article.get('publishedAt'):\n",
    "                continue\n",
    "                \n",
    "            date = datetime.strptime(article['publishedAt'][:10], '%Y-%m-%d')\n",
    "            title = article.get('title', '')\n",
    "            description = article.get('description', '')\n",
    "            \n",
    "            if not any([title, description]):\n",
    "                continue\n",
    "                \n",
    "            content = ' '.join(filter(None, [title, description]))\n",
    "            sentiment_scores = analyzer.polarity_scores(content)\n",
    "            \n",
    "            processed_data.append({\n",
    "                'date': date,\n",
    "                'sentiment': sentiment_scores['compound']\n",
    "            })\n",
    "            \n",
    "        except (KeyError, ValueError) as e:\n",
    "            print(f\"Error processing article: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not processed_data:\n",
    "        return pd.DataFrame(columns=['date', 'sentiment'])\n",
    "        \n",
    "    news_df = pd.DataFrame(processed_data)\n",
    "    return news_df.groupby('date')['sentiment'].mean().reset_index()\n",
    "\n",
    "def calculate_rsi(prices, periods=14):\n",
    "    # Relative Strength Index (RSI)\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=periods).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=periods).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def add_technical_indicators(df, window=10):\n",
    "    # Stock indicators for better predictions\n",
    "    df['volatility'] = df['Close'].rolling(window=window).std()\n",
    "    df['ma'] = df['Close'].rolling(window=window).mean()\n",
    "    df['volume_ma'] = df['Volume'].rolling(window=window).mean()\n",
    "    df['price_momentum'] = df['Close'].pct_change(periods=5)\n",
    "\n",
    "    # RSI\n",
    "    df['rsi'] = calculate_rsi(df['Close'], periods=10)\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    middle = df['Close'].rolling(window=window).mean()\n",
    "    std_dev = df['Close'].rolling(window=window).std()\n",
    "    \n",
    "    df['bb_middle'] = middle\n",
    "    df['bb_upper'] = middle + (std_dev * 2)\n",
    "    df['bb_lower'] = middle - (std_dev * 2)\n",
    "    \n",
    "    # MACD\n",
    "    exp1 = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['macd'] = exp1 - exp2\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    return df.ffill().bfill()\n",
    "\n",
    "def fetch_and_save_data(ticker, news_api_key):\n",
    "    try:\n",
    "        Path(\"data\").mkdir(exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nFetching data for {ticker}\")\n",
    "\n",
    "        # Get stock data\n",
    "        stock_data = yf.download(ticker, start=\"2021-01-01\", end=datetime.now().strftime('%Y-%m-%d'))\n",
    "        \n",
    "        if stock_data.empty:\n",
    "            return None, None\n",
    "            \n",
    "        # Fix the MultiIndex structure\n",
    "        if isinstance(stock_data.columns, pd.MultiIndex):\n",
    "            # Convert MultiIndex to single level using first level names\n",
    "            stock_data.columns = [col[0] for col in stock_data.columns]\n",
    "        \n",
    "        # Reset the index to get Date as a column\n",
    "        stock_data = stock_data.reset_index()\n",
    "        \n",
    "        # Convert Date to datetime\n",
    "        stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "        \n",
    "        print(f\"Processed columns: {stock_data.columns.tolist()}\")\n",
    "        \n",
    "        # Add technical indicators\n",
    "        stock_data = add_technical_indicators(stock_data)\n",
    "        \n",
    "        # Get and process news data\n",
    "        news_data = fetch_news(news_api_key, query=ticker)\n",
    "        processed_news = preprocess_news(news_data.get('articles', []))\n",
    "        \n",
    "        return stock_data, processed_news\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in fetch_and_save_data: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "\n",
    "    \n",
    "# Fetch and save data\n",
    "data_dict = {}\n",
    "for ticker in STOCK_SYMBOLS:\n",
    "    stock_data, news_data = fetch_and_save_data(ticker, NEWS_API_KEY)\n",
    "    if stock_data is not None and news_data is not None:\n",
    "        data_dict[ticker] = (stock_data, news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "define-constants",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(stock_data, news_data):\n",
    "    if stock_data is None or news_data is None:\n",
    "        print(\"Stock data or news data is None\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Handle MultiIndex columns\n",
    "        if isinstance(stock_data.columns, pd.MultiIndex):\n",
    "            stock_data.columns = [col[0] for col in stock_data.columns]\n",
    "            \n",
    "        # Ensure dates are datetime\n",
    "        stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "        news_data['date'] = pd.to_datetime(news_data['date'])\n",
    "        \n",
    "        # Merge on date\n",
    "        merged_data = pd.merge(\n",
    "            stock_data,\n",
    "            news_data,\n",
    "            left_on='Date',\n",
    "            right_on='date',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Fill missing sentiment with 0 (neutral sentiment)\n",
    "        merged_data['sentiment'] = merged_data['sentiment'].where(merged_data['sentiment'].notna(), 0)\n",
    "        \n",
    "        # Drop redundant date column\n",
    "        if 'date' in merged_data.columns:\n",
    "            merged_data.drop('date', axis=1, inplace=True)\n",
    "            \n",
    "        return merged_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in merge_data: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "# Process and merge data\n",
    "merged_data_dict = {}\n",
    "for ticker, (stock_data, news_data) in data_dict.items():\n",
    "    merged_data = merge_data(stock_data, news_data)\n",
    "    if merged_data is not None:\n",
    "        merged_data.to_csv(f\"data/cleaned_{ticker.lower()}_stock_data.csv\", index=False)\n",
    "        merged_data_dict[ticker] = merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fetch-news",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data with sentiment\n",
    "# 22 as the possible max of month\n",
    "def prepare_sets(stock_file, sequence_length=60):  # Consistently use 60 days\n",
    "    try:\n",
    "        stock_data = pd.read_csv(stock_file)\n",
    "        print(f\"Data columns available: {stock_data.columns.tolist()}\")\n",
    "        \n",
    "        # Ensure all required columns\n",
    "        feature_columns = [\n",
    "            'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "            'volatility', 'rsi', 'ma', 'volume_ma',\n",
    "            'price_momentum', 'bb_upper', 'bb_middle', 'bb_lower',\n",
    "            'macd', 'macd_signal', 'macd_hist',\n",
    "            'sentiment'\n",
    "        ]\n",
    "        \n",
    "        print(f\"Features needed: {feature_columns}\")\n",
    "        \n",
    "        # Scale all features together\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_data = scaler.fit_transform(stock_data[feature_columns])\n",
    "        \n",
    "        X_stock, X_sentiment, y = [], [], []\n",
    "        for i in range(len(scaled_data) - sequence_length):\n",
    "            # First 16 columns are stock features\n",
    "            X_stock.append(scaled_data[i:i + sequence_length, :16])\n",
    "            # Last column is sentiment\n",
    "            X_sentiment.append(scaled_data[i:i + sequence_length, -1:])\n",
    "            # Target is Close price\n",
    "            y.append(scaled_data[i + sequence_length, 3])\n",
    "        \n",
    "        X_stock = np.array(X_stock)\n",
    "        X_sentiment = np.array(X_sentiment)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        print(f\"Final shapes: X_stock={X_stock.shape}, X_sentiment={X_sentiment.shape}\")\n",
    "        \n",
    "        return X_stock, X_sentiment, y, scaler\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "\n",
    "\n",
    "# Load and prepare data\n",
    "prepared_data = {}\n",
    "for symbol in STOCK_SYMBOLS:\n",
    "    stock_file = f\"data/cleaned_{symbol.lower()}_stock_data.csv\"\n",
    "    print(f\"\\nProcessing sets for {symbol}\")\n",
    "    \n",
    "    X_stock, X_sentiment, y, scaler = prepare_sets(stock_file, sequence_length=60)\n",
    "    if all(v is not None for v in [X_stock, X_sentiment, y, scaler]):\n",
    "        prepared_data[symbol] = (X_stock, X_sentiment, y, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fetch-save-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(sequence_length, n_features):\n",
    "    stock_input = Input(shape=(sequence_length, n_features))\n",
    "    \n",
    "    # Multiple time scales\n",
    "    conv1_daily = Conv1D(64, kernel_size=1, padding='causal', activation='relu')(stock_input)\n",
    "    conv1_weekly = Conv1D(64, kernel_size=2, padding='causal', dilation_rate=2, activation='relu')(stock_input)\n",
    "    conv1_monthly = Conv1D(64, kernel_size=5, padding='causal', dilation_rate=4, activation='relu')(stock_input)\n",
    "    \n",
    "    # Reduce dims\n",
    "    pool_size = 2\n",
    "    pool_daily = MaxPooling1D(pool_size=pool_size)(conv1_daily)\n",
    "    pool_weekly = MaxPooling1D(pool_size=pool_size)(conv1_weekly)\n",
    "    pool_monthly = MaxPooling1D(pool_size=pool_size)(conv1_monthly)\n",
    "    \n",
    "    # Merge\n",
    "    cnn_merged = concatenate([pool_daily, pool_weekly, pool_monthly], axis=2)\n",
    "    \n",
    "    #  LSTM - Long term dependencies\n",
    "    lstm = Bidirectional(LSTM(100, return_sequences=True))(cnn_merged)\n",
    "    \n",
    "    # Self-attention\n",
    "    attention = Dense(1, activation='tanh')(lstm)\n",
    "    attention = Flatten()(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = RepeatVector(lstm.shape[-1])(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "    \n",
    "    sent_lstm = multiply([lstm, attention])\n",
    "    sent_lstm = LSTM(100)(sent_lstm)\n",
    "    \n",
    "    # Sentiment\n",
    "    sentiment_input = Input(shape=(sequence_length, 1))\n",
    "    sent_dense = TimeDistributed(Dense(32, activation='relu'))(sentiment_input)\n",
    "    sent_lstm_2 = LSTM(50)(sent_dense)\n",
    "    \n",
    "    combined = concatenate([Flatten()(cnn_merged), sent_lstm, sent_lstm_2])\n",
    "    \n",
    "    dense1 = Dense(128, activation='relu')(combined)\n",
    "    dense2 = Dense(64, activation='relu')(dense1)\n",
    "    residual = concatenate([dense1, dense2])\n",
    "    \n",
    "    dropout = Dropout(0.3)(residual)\n",
    "    output = Dense(1)(dropout)\n",
    "    \n",
    "    model = Model(inputs=[stock_input, sentiment_input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=huber_loss(),\n",
    "        metrics=['mse', 'mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model_with_validation(X_stock, X_sentiment, y, symbol):\n",
    "    # Split data into train and validation (test not used in training)\n",
    "    train_size = int(len(X_stock) * 0.7)\n",
    "    val_size = int(len(X_stock) * 0.15)\n",
    "    \n",
    "    X_stock_train = X_stock[:train_size]\n",
    "    X_stock_val = X_stock[train_size:train_size+val_size]\n",
    "    \n",
    "    # Similar splits for sentiment and target\n",
    "    X_sentiment_train = X_sentiment[:train_size]\n",
    "    X_sentiment_val = X_sentiment[train_size:train_size+val_size]\n",
    "    \n",
    "    y_train = y[:train_size]\n",
    "    y_val = y[train_size:train_size+val_size]\n",
    "    \n",
    "    model = build_model(X_stock.shape[1], X_stock.shape[2])\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            mode='min'\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=0.0001\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            f'models/{symbol}_best_model.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train with cross-validation\n",
    "    history = model.fit(\n",
    "        [X_stock_train, X_sentiment_train],\n",
    "        y_train,\n",
    "        validation_data=([X_stock_val, X_sentiment_val], y_val),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "\n",
    "models = {}\n",
    "scalers = {}\n",
    "results = {}\n",
    "\n",
    "for symbol, (X_stock, X_sentiment, y, scaler) in prepared_data.items():\n",
    "    # Split data\n",
    "    split_idx = int(0.8 * len(X_stock))\n",
    "    train_data = {\n",
    "        'X_stock': X_stock[:split_idx],\n",
    "        'X_sentiment': X_sentiment[:split_idx],\n",
    "        'y': y[:split_idx]\n",
    "    }\n",
    "    test_data = {\n",
    "        'X_stock': X_stock[split_idx:],\n",
    "        'X_sentiment': X_sentiment[split_idx:],\n",
    "        'y': y[split_idx:]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Training\n",
    "        model, history = train_model_with_validation(\n",
    "            train_data['X_stock'], \n",
    "            train_data['X_sentiment'], \n",
    "            train_data['y'],\n",
    "            symbol\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss = model.evaluate(\n",
    "            [test_data['X_stock'], test_data['X_sentiment']],\n",
    "            test_data['y'],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        results[symbol] = {\n",
    "            'test_loss': test_loss,\n",
    "            'model': model,\n",
    "            'scaler': scaler,\n",
    "            'test_data': test_data\n",
    "        }\n",
    "        \n",
    "        print(f\"{symbol} Test Loss: {test_loss}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {symbol}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data-with-sentiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"models\").mkdir(exist_ok=True)\n",
    "\n",
    "for symbol, result in results.items():\n",
    "    with open(f\"models/{symbol}_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(result['model'], f)\n",
    "    with open(f\"models/{symbol}_scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(result['scaler'], f)\n",
    "    print(f\"Saved model and scaler for {symbol}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
