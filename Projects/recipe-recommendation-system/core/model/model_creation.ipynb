{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.13.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting ingredient-parser-nlp\n",
      "  Using cached ingredient_parser_nlp-1.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting nltk>=3.9.1 (from ingredient-parser-nlp)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting python-crfsuite (from ingredient-parser-nlp)\n",
      "  Using cached python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting pint>=0.24.4 (from ingredient-parser-nlp)\n",
      "  Using cached Pint-0.24.4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting numpy>=1.19.5 (from scikit-learn)\n",
      "  Using cached numpy-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting click (from nltk>=3.9.1->ingredient-parser-nlp)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>=3.9.1->ingredient-parser-nlp)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk>=3.9.1->ingredient-parser-nlp)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: platformdirs>=2.1.0 in /home/cristi/DS/.venv/lib/python3.12/site-packages (from pint>=0.24.4->ingredient-parser-nlp) (4.3.6)\n",
      "Collecting flexcache>=0.3 (from pint>=0.24.4->ingredient-parser-nlp)\n",
      "  Using cached flexcache-0.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting flexparser>=0.4 (from pint>=0.24.4->ingredient-parser-nlp)\n",
      "  Using cached flexparser-0.4-py3-none-any.whl.metadata (18 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached beautifulsoup4-4.13.1-py3-none-any.whl (185 kB)\n",
      "Using cached ingredient_parser_nlp-1.3.2-py3-none-any.whl (827 kB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (145 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached numpy-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "Using cached Pint-0.24.4-py3-none-any.whl (302 kB)\n",
      "Using cached scipy-1.15.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.2 MB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached flexcache-0.3-py3-none-any.whl (13 kB)\n",
      "Using cached flexparser-0.4-py3-none-any.whl (27 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, threadpoolctl, soupsieve, regex, python-crfsuite, numpy, joblib, idna, click, charset-normalizer, certifi, scipy, requests, nltk, flexparser, flexcache, beautifulsoup4, scikit-learn, pint, ingredient-parser-nlp\n",
      "Successfully installed beautifulsoup4-4.13.1 certifi-2025.1.31 charset-normalizer-3.4.1 click-8.1.8 flexcache-0.3 flexparser-0.4 idna-3.10 ingredient-parser-nlp-1.3.2 joblib-1.4.2 nltk-3.9.1 numpy-2.2.2 pint-0.24.4 python-crfsuite-0.9.11 regex-2024.11.6 requests-2.32.3 scikit-learn-1.6.1 scipy-1.15.1 soupsieve-2.6 threadpoolctl-3.5.0 tqdm-4.67.1 typing-extensions-4.12.2 urllib3-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 ingredient-parser-nlp scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/cristi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from ingredient_parser import parse_ingredient\n",
    "import concurrent.futures\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "url = \"https://www.allrecipes.com/recipes-a-z-6735880\"\n",
    "\n",
    "recipe_links_set = set() # links to actual recipes\n",
    "sublinks_set = set() # links to other pages that contain recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sublinks_retrieval(href):\n",
    "    response = requests.get(href, headers=headers)\n",
    "    if (response.status_code == 200):\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n",
    "        return\n",
    "\n",
    "    unordered_list = soup.find_all('ul', class_=\"comp mntl-taxonomy-nodes__list mntl-block\")\n",
    "\n",
    "    if len(unordered_list) > 0: \n",
    "        soup = BeautifulSoup(str(unordered_list[0]), 'html.parser')\n",
    "        list_items = soup.find_all('li')\n",
    "        \n",
    "        for li in list_items:\n",
    "            soup = BeautifulSoup(str(li), 'html.parser')\n",
    "            item = soup.find_all('a')\n",
    "\n",
    "            for link in item:\n",
    "                href = link.get('href')\n",
    "                if href not in sublinks_set:\n",
    "                    sublinks_set.add(href)\n",
    "                    sublinks_retrieval(href)\n",
    "\n",
    "    # get links to recipes from the current page\n",
    "    cards_hyperlink_elems = soup.find_all('a', class_=\"comp mntl-card-list-items mntl-universal-card mntl-document-card mntl-card card card--no-image\")\n",
    "    for hyperlink_elem in cards_hyperlink_elems:\n",
    "        soup = BeautifulSoup(str(hyperlink_elem), 'html.parser')\n",
    "        fav_div = soup.find_all('div', class_=\"comp card__favorite mm-myrecipes-favorite\")\n",
    "        if len(fav_div) > 0:\n",
    "            href = hyperlink_elem.get('href')\n",
    "            if href not in recipe_links_set:\n",
    "                recipe_links_set.add(href)\n",
    "\n",
    "def scrape_links_to_recipes():\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"Scrapping started\")\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('a', class_=\"mntl-link-list__link text-body-100 global-link text-body-100 global-link\")\n",
    "        unique_links = set(links)\n",
    "\n",
    "        cnt = 0\n",
    "        for link in unique_links:\n",
    "            href = link.get('href')\n",
    "            if href not in sublinks_set:\n",
    "                print(f\"Scrapping {href} total_recipes {len(recipe_links_set)}\")\n",
    "                sublinks_set.add(href)\n",
    "                sublinks_retrieval(href)\n",
    "            else:\n",
    "                print(f\"Already scrapped {href}\")                \n",
    "    \n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n",
    "\n",
    "    with open(\"./recipe_links.txt\", \"w\") as file:\n",
    "        for item in recipe_links_set:\n",
    "            file.write(f\"{item}\\n\")\n",
    "\n",
    "    print(\"Scrapping finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_recipe(link):\n",
    "    \"\"\"Scrapes a single recipe given a link.\"\"\"\n",
    "    recipe_data = {}\n",
    "    print(f\"Scraping: {link.strip()}\") # Print which link is being scraped\n",
    "    try:\n",
    "        response = requests.get(link.strip(), headers=headers, allow_redirects=True, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Cannot access the recipe link: {link}\")\n",
    "            return None  # Return None if there's an issue\n",
    "\n",
    "        recipe_data['link'] = link.strip()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        main_html_tag = soup.find('main', class_=\"loc main\")\n",
    "\n",
    "        if not main_html_tag:\n",
    "            print(f\"Main content not found for link: {link}\")\n",
    "            return None\n",
    "\n",
    "        main_soup = BeautifulSoup(str(main_html_tag), \"html.parser\")\n",
    "\n",
    "        # Title\n",
    "        title_element = main_soup.find('h1', class_='article-heading')\n",
    "        recipe_data['title'] = title_element.text.strip() if title_element else \"Title not found.\"\n",
    "\n",
    "        # Description\n",
    "        description_element = main_soup.find('p', class_='article-subheading')\n",
    "        recipe_data['description'] = description_element.text.strip() if description_element else \"Description not found.\"\n",
    "\n",
    "        # Total Time\n",
    "        total_time_element = main_soup.find('div', class_='mm-recipes-details__label', string='Total Time:')\n",
    "        if total_time_element:\n",
    "            total_time_value_element = total_time_element.find_next_sibling('div', class_='mm-recipes-details__value')\n",
    "            recipe_data['total_time'] = total_time_value_element.text.strip() if total_time_value_element else \"Total Time value not found.\"\n",
    "        else:\n",
    "            recipe_data['total_time'] = \"Total Time label not found.\"\n",
    "\n",
    "        # Ingredients\n",
    "        ingredients = []\n",
    "        ingredients_div = main_soup.find('div', class_=\"comp mm-recipes-structured-ingredients\")\n",
    "        if ingredients_div:\n",
    "            aux_soup = BeautifulSoup(str(ingredients_div), \"html.parser\")\n",
    "            ingredients_ul = aux_soup.find('ul', class_=\"mm-recipes-structured-ingredients__list\")\n",
    "            if ingredients_ul:\n",
    "                aux_soup = BeautifulSoup(str(ingredients_ul), \"html.parser\")\n",
    "                ingredients_li = aux_soup.find_all('li', class_=\"mm-recipes-structured-ingredients__list-item\")\n",
    "                for ingredient_li in ingredients_li:\n",
    "                    aux_soup = BeautifulSoup(str(ingredient_li), \"html.parser\")\n",
    "                    p_tag = aux_soup.find('p')\n",
    "                    for child in p_tag.children:\n",
    "                        if child.name == 'span' and child.get('data-ingredient-name') == 'true':\n",
    "                            try:\n",
    "                                parsed_ingredient = parse_ingredient(child.text.strip().lower())\n",
    "                                if parsed_ingredient.name and len(parsed_ingredient.name.text) > 1:\n",
    "                                    ingredients.append(parsed_ingredient.name.text)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error parsing ingredient: {child.text.strip().lower()} - Error: {e}\")\n",
    "        recipe_data['ingredients'] = ingredients\n",
    "\n",
    "        # Steps\n",
    "        steps = []\n",
    "        for li in main_soup.find_all('li', class_='mntl-sc-block-group--LI'):\n",
    "            p = li.find('p', class_='mntl-sc-block-html')\n",
    "            if p:\n",
    "                step_text = p.text.strip()\n",
    "                steps.append(step_text)\n",
    "        recipe_data['steps'] = steps\n",
    "\n",
    "        return recipe_data\n",
    "\n",
    "    except requests.exceptions.RequestException as e: # Catch request errors\n",
    "        print(f\"Request error for {link}: {e}\")\n",
    "        return None\n",
    "    except Exception as e: # Catch any other error\n",
    "        print(f\"Error processing {link}: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_all_recipes():\n",
    "    all_recipes = []\n",
    "\n",
    "    with open('recipe_links.txt', 'r') as file:\n",
    "        links = [link.strip() for link in file]  # Read all links into memory\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor: # Use ThreadPoolExecutor\n",
    "        futures = [executor.submit(scrape_recipe, link) for link in links]\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures): # Process results as they complete\n",
    "            recipe_data = future.result()\n",
    "            if recipe_data:\n",
    "                all_recipes.append(recipe_data)\n",
    "\n",
    "    with open(\"recipes.json\", 'w', encoding=\"utf-8\") as file:\n",
    "        json.dump(all_recipes, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(\"Scraping and saving to JSON complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def sanitize_ingredient(ingredient):\n",
    "\n",
    "    ingredient = ingredient.lower().strip()\n",
    "    ingredient = re.sub(r\"[\\u00AE\\u2122]\", \"\", ingredient).strip()  # Remove registered and trademark symbols\n",
    "    ingredient = re.sub(r\" \\(.+?\\)\", \"\", ingredient).strip()  # Remove parenthetical info\n",
    "    ingredient = re.sub(r\"[^a-zA-Z0-9\\s-]\", \"\", ingredient).strip()  # Remove special characters (except hyphen)\n",
    "    ingredient = re.sub(r\"\\s+\", \" \", ingredient).strip()  # Remove extra spaces\n",
    "    ingredient = re.sub(r\"^(fresh|dried|frozen|canned|chopped|sliced|ground|minced|diced|cooked|raw|or|and|with|in|of|the|a|an|all-natural|all-purpose|all-beef|100%|%|number|#|\\$|\\d+(?:st|nd|rd|th)?(?:-less)?(?:-fat)?(?:-free)?)\\s*\", \"\", ingredient).strip()  # Remove prefixes, numbers, units, etc.\n",
    "    ingredient = lemmatizer.lemmatize(ingredient)  # Lemmatize\n",
    "    ingredient = ingredient.strip()  # Remove any remaining whitespace\n",
    "    return ingredient\n",
    "\n",
    "def create_sanitized_datasets():\n",
    "    sanitized_ingredients_set = set()\n",
    "\n",
    "    with open(\"recipes.json\", 'r', encoding='utf-8') as f:\n",
    "        recipes_data = json.load(f)\n",
    "    \n",
    "    for recipe in recipes_data:\n",
    "        sanitized_recipe_ingredients = []\n",
    "        for ingredient in recipe['ingredients']:\n",
    "            result = sanitize_ingredient(ingredient)\n",
    "            if not result:\n",
    "                print(ingredient)\n",
    "                continue\n",
    "            \n",
    "            sanitized_recipe_ingredients.append(result)\n",
    "        \n",
    "        recipe['sanitized_ingredients'] = sanitized_recipe_ingredients\n",
    "        for sanitized_ingredient in sanitized_recipe_ingredients:\n",
    "            sanitized_ingredients_set.add(sanitized_ingredient)\n",
    "    \n",
    "    with open(\"sanitized_ingredients.txt\", 'w') as f:\n",
    "        for sanitized_ingredient in sanitized_ingredients_set:\n",
    "            f.write(f\"{sanitized_ingredient}\\n\")\n",
    "    \n",
    "    with open(\"sanitized_recipes.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(recipes_data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_matrix_files(recipes_data):\n",
    "    recipe_ingredient_strings = []\n",
    "    for recipe in recipes_data:\n",
    "        ingredient_names = [ingredient.lower().strip() for ingredient in recipe.get('sanitized_ingredients', [])] # Basic lowercasing and stripping\n",
    "        recipe_ingredient_strings.append(\" \".join(ingredient_names))\n",
    "\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf.fit_transform(recipe_ingredient_strings)\n",
    "\n",
    "    with open('model_file', 'wb') as f:\n",
    "        pickle.dump(tfidf, f)\n",
    "\n",
    "    with open('matrix_file', 'wb') as f:\n",
    "        pickle.dump(tfidf_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_ingredients(all_ingredients, target_ingredients):\n",
    "    related_ingredients = []\n",
    "    for ingredient in all_ingredients:\n",
    "        for target_ingredient in target_ingredients:\n",
    "            if target_ingredient.lower() in ingredient.lower():  # Case-insensitive check\n",
    "                related_ingredients.append(ingredient)\n",
    "    return related_ingredients\n",
    "\n",
    "def calculate_similarity(user_ingredients, loaded_tfidf, loaded_tfidf_matrix):\n",
    "    user_ingredient_string = \" \".join([ingredient.lower().strip() for ingredient in user_ingredients])\n",
    "    user_vector = loaded_tfidf.transform([user_ingredient_string]) # Use the loaded tfidf model\n",
    "    similarity_scores = cosine_similarity(user_vector, loaded_tfidf_matrix) # Use the loaded matrix\n",
    "    return similarity_scores\n",
    "\n",
    "def load_and_test_model(model_file, matrix_file, ingredients_set, recipes):\n",
    "    try:\n",
    "        with open(model_file, 'rb') as f:\n",
    "            loaded_tfidf = pickle.load(f)\n",
    "        with open(matrix_file, 'rb') as f:\n",
    "            loaded_tfidf_matrix = pickle.load(f)\n",
    "        print(\"Model and matrix loaded successfully.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Model or matrix file not found.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or matrix: {e}\")\n",
    "        return\n",
    "\n",
    "    user_ingredients = [\"onion\", \"butter\", \"eggs\", \"bread\", \"milk\"]  # Example user input\n",
    "    enriched_user_ingredients = lookup_ingredients(ingredients_set, user_ingredients)\n",
    "    similarity_scores = calculate_similarity(enriched_user_ingredients, loaded_tfidf, loaded_tfidf_matrix)\n",
    "\n",
    "    N = 5  # Number of recommendations\n",
    "    top_n_indices = similarity_scores.argsort()[0][::-1][:N]\n",
    "\n",
    "    recommended_recipes = [recipes[i] for i in top_n_indices]\n",
    "\n",
    "    for recipe in recommended_recipes:\n",
    "        print(recipe['title'])\n",
    "        print(recipe['ingredients'])\n",
    "        print(recipe['link'])\n",
    "        print(\"-\" * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_set = set()\n",
    "\n",
    "create_sanitized_datasets()\n",
    "\n",
    "with open(\"sanitized_recipes.json\", 'r', encoding='utf-8') as f:\n",
    "    recipes_data = json.load(f)\n",
    "\n",
    "create_model_and_matrix_files(recipes_data)\n",
    "\n",
    "# load_and_test_model(\"model_file\", \"matrix_file\", ingredients_set, recipes_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
